---
layout: post
title: "「MIT 6.828」MIT 6.828 Fall 2018 lab6"
subtitle: " Lab6 终于到最后一个part了"
date:        2022-05-17 21:06:00
author:  "许大仙"
catalog: true
tags:
    - 系统
---

最近的科研告一段落了，实话实话，不太有多少成绩，完全算不是满意，不过过程还是比较快乐的。

马上要去实习了，所以把脱了这么久的OS lab做完吧。

然后再啃啃ULK和《Linux内核设计与实现》吧。

# Lab 6: Network Driver (default final project)

这是最后一个独立完成的lab了。

OS不能没有network stack【no self respecting OS should go without a network stack】，在这个实验中，我们要写一个E1000网卡的驱动。

>The card will be based on the Intel 82540EM chip, also known as the E1000.

Commit lab5中实现的内容，merge lab5到lab6 分支，并开始实验吧。

网卡驱动程序不足以让您的操作系统连接到 Internet。因此，在lab6 ，我们于新的 `net/`目录以及`kern/`中的新文件中为您提供了网络栈和网络服务器（a network stack and a network server）的code。

除了编写驱动程序之外，您还需要创建一个系统调用接口来访问您的驱动程序。您将实现missing的部分网络服务器代码以在network stack和driver之间传输数据包。您还将通过完成一个web服务器将所有内容联系在一起。使用自实现的Web服务器，您将能够从lab5中的文件系统内通过网络提取文件。

许多内核设备驱动程序代码几乎需要自己从头开始编写，并且这个实验提供的指导比以前的实验少得多：没有框架文件，没有一成不变的系统调用接口，许多设计决策都由您决定。出于这个原因，建议您在开始任何单独的练习之前阅读整个lab的描述。

## QEMU's virtual network

我们将使用 QEMU 的user mode下的network stack，因为它不需要管理权限即可运行。关于QEMU user-net的[信息文档在这里](http://wiki.qemu.org/download/qemu-doc.html#Using-the-user-mode-network-stack)。我们更新了 makefile 以启用 QEMU user-mode network stack和E1000 虚拟网卡。

默认情况下，QEMU 提供了一个运行在 IP 10.0.2.2 上的虚拟路由器，并将分配给 JOS 的 IP 地址为 10.0.2.15。为了简单起见，我们将这些默认值硬编码到`net/ns.h`中的network server内。

通过使用`-net user` 选项（即默认配置 ），QEMU将完全使用user-mode network stack。具体的虚拟网络配置如下：

```
guest (10.0.2.15)  <------>  Firewall/DHCP server <-----> Internet
                      |          (10.0.2.2)
                      |
                      ---->  DNS server (10.0.2.3)
                      |
                      ---->  SMB server (10.0.2.4)
```

The QEMU VM behaves as if it was behind a firewall which blocks all incoming connections.

虽然 QEMU's virtual network 允许 JOS 与 Internet 建立任意连接，但 JOS 的 10.0.2.15 地址在 QEMU 内部运行的虚拟网络之外没有任何意义（即 QEMU 充当 NAT），因此我们不能直接从外部甚至是host，连接到在JOS中运行的network server。为了解决这个问题，我们将 QEMU 配置为在主机的某个端口上运行一个服务器，该服务器简单地连接到 JOS 中的某个端口，并在您的真实主机和虚拟网络之间来回传输数据。【根据上述提到的QEMU文档，我们得知这种从host某个端口到QEMU guest的重定向可以通过`-netdev user,hostfwd=...`来实现】

我们将在端口 7 (echo) 和 80 (http) 上运行 JOS servers。为避免共享 Athena 机器【和课程相关】上的冲突，makefile 会根据您的用户 ID 生成转发端口。要找出 QEMU 在您的开发主机上转发到哪些端口，请运行`make which-ports`。为方便起见，makefile 还提供了`make nc-7`和`make nc-80`，它允许您直接与在终端的这些端口上运行的服务器进行交互。（这仅用于通过nc连接到正在运行的 QEMU 实例，您必须提前单独启动 QEMU。）

### Packet Inspection

makefile 还配置QEMU的network stack以将所有传入和传出的数据包记录到/lab目录中的`qemu.pcap`。

要获取捕获数据包的十六进制/ASCII 转储，请使用`tcpdump`，如下所示：

```
tcpdump -XXnr qemu.pcap
```

同时您可以使用Wireshark以图形方式检查 pcap 文，Wireshark可支持解码和检查数百种网络协议。

### Debugging the E1000

我们很幸运能够使用硬件仿真。Since the E1000 is running in software, 因此仿真的E1000可以以用户能够读懂的方式将内部状态和遇到的任何bug报告给我们。通常，对于使用裸机编写的驱动程序开发人员来说，这是几乎不可得的。

E1000 可以产生大量调试输出，为此您必须启用特定的日志记录通道（logging channels）。Some channels you might find useful are:

<img src="E:\github_repository\spidermana.github.io\img\assets\img\image-20220517220028916.png" alt="image-20220517220028916" style="zoom:50%;" />

例如，要启用“tx”和“txerr”日志记录，请使用`make E1000_DEBUG=tx,txerr ....`

> 注意： `E1000_DEBUG`标志仅适用于 6.828 版本的 QEMU。

You can take debugging using software emulated hardware one step further。也就是说，如果您遇到困难并且不明白为什么 E1000 没有按照您期望的方式运作，您可以在`hw/net/e1000.c`中查看 QEMU 的 E1000 实现。

## The Network Server

Writing a network stack from scratch is hard work. Instead, we will be using [IwIP](https://savannah.nongnu.org/projects/lwip/), 一个轻量级的开源TCP/IP协议套件（其中包含了一个协议栈）。在本实验中，lwIP可以看做是一个实现了 BSD 套接字接口并具有数据包输入端口和数据包输出端口的黑盒。

一个网络服务实际上是四个环境的组合：

- core network server environment (includes socket call dispatcher and lwIP)
- input environment
- output environment
- timer environment

下图显示了不同的环境及其关系。该图显示了整个系统，包括设备驱动程序，稍后将介绍。在本实验中，您将实现绿色高亮的部分。

<img src="https://pdos.csail.mit.edu/6.828/2018/labs/lab6/ns.png" alt="Network server architecture" style="zoom: 67%;" />

## The Core Network Server Environment

核心网络服务组件（Core Network Server Environment）是由socket call dispatcher和network stack（即这里IwIP）组成的。套接字调用调度程序（socket call dispatcher）的工作方式与文件服务器完全相同。用户环境使用存根stubs （定义在`lib/nsipc.c`中）将 IPC messages发送到核心网络服务组件。

如果您查看 `lib/nsipc.c`，您会看到我们找到核心网络服务组件的方式与我们找到文件服务器的方式相同：在系统初始化的时候，调用了`i386_init`——使用 NS_TYPE_NS 创建了 NS 环境，此后通过扫描`envs`，寻找这种特殊的环境类型（nsenv）。对于每个用户环境的 IPC请求，网络服务组件中的调度程序（socket call dispatcher）会代表用户调用 lwIP协议栈提供的相应 BSD 套接字接口函数。

```c
//我们经过了一个又一个实验
void
i386_init(void)
{
	// Initialize the console.
	// Can't call cprintf until after we do this!
	cons_init();
	cprintf("6828 decimal is %o octal!\n", 6828);

	// Lab 2 memory management initialization functions
	mem_init();
	// Lab 3 user environment initialization functions
	env_init();
	trap_init();
	// Lab 4 multiprocessor initialization functions
	mp_init();
	lapic_init();
	// Lab 4 multitasking initialization functions
	pic_init();

	// Lab 6 hardware initialization functions
	time_init();
	pci_init();
	
	// Acquire the big kernel lock before waking up APs
	// Your code here:
	lock_kernel();

	// Starting non-boot CPUs
	boot_aps();

	// Start fs. 【可以看到这里在初始化文件系统环境】
	ENV_CREATE(fs_fs, ENV_TYPE_FS);

#if !defined(TEST_NO_NS)
	// Start ns.【类似地初始化了网络服务组件的环境】
	ENV_CREATE(net_ns, ENV_TYPE_NS);
#endif

	// Schedule and run the first user environment!
	sched_yield();
}

//最后ENV_CREATE会调用env.c中的env_create(uint8_t *binary, enum EnvType type)，为特定的binary分配env结构，然后加载这个binary到特定的虚拟进程空间中。

//我们可以在 `lib/nsipc.c`中看到寻找核心网络服务组件相关的代码如下：
static int
nsipc(unsigned type)
{
    static envid_t nsenv;
	if (nsenv == 0)
		nsenv = ipc_find_env(ENV_TYPE_NS);
    /* …… */
}
```

常规用户环境下，是不会直接使用调用`nsipc_*`的【即lwIP协议栈提供的BSD套接字接口函数，可见于`lib/nsipc.c`，如nsipc_accept, nsipc_bind之类的】。相反，正常用户程序会使用`lib/sockets.c`中的函数，它提供了一个基于文件描述符的套接字 API。因此，用户环境可以通过文件描述符引用套接字，就像它们引用磁盘文件一样。有许多操作（`connect`,`accept`等）是特定于套接字的，但是对于另外一些操作如`read`, `write`等则通过lib/fd.c中正常的文件描述符设备调度代码进行。此外，就像文件服务器如何为所有打开的文件维护内部唯一ID一样，lwIP 也为所有打开的套接字生成唯一 ID。在文件服务器和网络服务器中，我们使用存储在`struct Fd`中信息将每个环境/进程的文件描述符映射到这些唯一的 ID 空间。

尽管看起来文件服务器和网络服务器的 IPC 调度程序的行为相同，但还是有一个关键的区别。<u>BSD socket calls like `accept` and `recv` 可以无限制的阻塞。</u>如果调度器让 lwIP 执行这些阻塞调用之一，调度器也会阻塞，整个系统一次只能有一个未完成的网络调用。由于这是不可接受的，网络服务组件会使用用户级线程（ user-level threading）来避免阻塞整个服务器环境（server environment）。<u>对于每个传入的 IPC 消息</u>，调度程序创建一个网络线程并在新创建的线程中处理请求。如果线程阻塞，则只有该网络线程进入睡眠状态，而其他线程继续运行。【因此IPC dispatcher基本上都是多线程程序】

除了核心网络环境之外，还有三个辅助环境。除了接受来自用户应用程序的消息外，核心网络环境的调度程序还接受来自输入环境和计时器环境的消息。

## The Output Environment

在服务用户环境套接字调用时，lwIP 将生成数据包交由网卡传输。具体来说，LwIP会将每个包传递给output helper environment ，即通过将数据包附加在 `NSREQ_OUTPUT` IPC 消息的page参数中，完成传递。The output environment会负责接受这些消息并通过我们即将创建的系统调用接口将数据包转发到设备驱动程序。

## The Input Environment

网卡从网络中收到的数据包需要注入lwIP协议栈中，这就需要the input helper environment的帮助。

对于设备驱动程序接收到的每个数据包， the input environment将数据包pull出内核空间（使用您将实现的内核系统调用）并用过`NSREQ_INPUT`IPC 消息将数据包发送到核心网络服务环境（core network server environment）。

数据包输入功能与核心网络服务环境分离，因为 JOS 很难同时accept IPC 消息，并轮询或等待来自设备驱动程序的数据包。我们在 JOS 中没有实现`select`系统调用，以允许environments监视多个输入源，以识别哪些输入已准备好被处理。

如果您查看`net/input.c`和`net/output.c`，您会发现两者都需要实现。这主要是因为实现取决于你的系统调用接口。在实现驱动程序和系统调用接口后，您将为这两个辅助环境（即input/output environments）编写代码。

## The Timer Environment

定时器环境定期向核心网络服务组件发送`NSREQ_TIMER`类型的消息，通知它定时器的时间已到（expired）。lwIP使用来自该线程/环境的计时器消息来实现各种网络超时。

# Part A: Initialization and transmitting packets

你的内核是没有时间概念的，所以我们需要为其添加。当前有一个由硬件每 10ms 产生一次的时钟中断。在每次时钟中断时，我们可以增加一个变量来指示时间走了10ms。这是在`kern/time.c`中实现的，但尚未完全集成到您的内核中。

>**Exercise 1.** Add a call to `time_tick` for every clock interrupt in `kern/trap.c`. Implement `sys_time_msec` and add it to `syscall` in `kern/syscall.c` so that user space has access to the time.



用于make INIT_CFLAGS=-DTEST_NO_NS run-testtime测试您的时间码。您应该会看到环境以 1 秒的间隔从 5 倒计时。“-DTEST_NO_NS”禁用启动网络服务器环境，因为它会在实验室的此时发生恐慌。



编写驱动程序需要深入了解硬件和呈现给软件的接口。实验文本将提供有关如何与 E1000 交互的高级概述，但您需要在编写驱动程序时大量使用英特尔的手册。



# Part B: Receiving packets and the web server
